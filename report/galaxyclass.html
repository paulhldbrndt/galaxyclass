<!DOCTYPE html>
<html lang="en" data-theme="light"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><link rel="preload" href="https://assets.vercel.com/raw/upload/v1587415301/fonts/2/inter-var-latin.woff2" as="font" type="font/woff2" crossorigin="anonymous"><title>Galaxy morphology classification with Convolutional Neural Networks</title><meta name="og:title" content="Galaxy morphology classification with Convolutional Neural Networks"><meta name="description" content="Galaxy morphology classification with Convolutional Neural Networks"><meta name="og:description" content="Galaxy morphology classification with Convolutional Neural Networks"><meta name="og:image" content=""><meta name="next-head-count" content="18"><link rel="preload" href="./files/eb6e8645206fe662.css" as="style"><link rel="stylesheet" href="./files/eb6e8645206fe662.css" data-n-g=""><link rel="preload" href="./files/721083576eb79b1c.css" as="style"><link rel="stylesheet" href="./files/721083576eb79b1c.css" data-n-p=""><noscript data-n-css=""></noscript></head><body><div id="__next" data-reactroot=""><div class="page_wrapper__LU8o_"><nav class="header_nav__dsvPb"></nav><main class="page_main__qrCzj"><article><span class="post_date__GBVoG">Paul Hildebrandt</span><div class="heading"><h1 class="post_title__A0uei">Galaxy morphology classification with Convolutional Neural Networks</h1></div><p><img src="./files/galaxyclass-header.png" alt="galaxyclass"></p>
<p>This is a deep learning project that I completed for <a href="https://www.ph.tum.de/academics/org/cc/mh/PH0101/" target="_blank" rel="noopener noreferrer" class="link_underline__J1V8J">PH0101</a> as part of my undergraduate in physics. I’m going to walk you through the entire workflow for this project, including exploratory data analysis, image preprocessing, and finally building a convolutional neural network (CNN) with TensorFlow and the Keras API.</p>
<p><a href="notebook.html" target="_blank" rel="noopener noreferrer" class="link_underline__J1V8J">View the Jupyter Notebook (HTML version)</a>, which contains the full source code as well as my learning notes.</p>
<h3><a href="#index" id="index" class="header-link">Index</a></h3><ol>
<li><a href="#motivation" class="link_underline__J1V8J">Motivation</a></li><li><a href="#project-background" class="link_underline__J1V8J">Project Background</a></li><li><a href="#exploratory-data-analysis" class="link_underline__J1V8J">Exploratory Data Analysis</a></li><li><a href="#benchmarking" class="link_underline__J1V8J">Benchmarking</a></li><li><a href="#preprocessing" class="link_underline__J1V8J">Preprocessing</a></li><li><a href="#convolutional-neural-network" class="link_underline__J1V8J">Convolutional Neural Network</a></li><li><a href="#results" class="link_underline__J1V8J">Results</a></li><li><a href="#installation" class="link_underline__J1V8J">Installation</a></li><li><a href="#usage" class="link_underline__J1V8J">Usage</a></li><li><a href="#conclusion" class="link_underline__J1V8J">Conclusion</a></li><li><a href="#references" class="link_underline__J1V8J">References</a></li></ol>
<h2><a href="#motivation" id="motivation" class="header-link">Motivation</a></h2><p>As the universe continues to expand, questions remain about how galaxies formed and evolved. To solve this puzzle of cosmic structure, understanding the distribution and types of galaxies as a function of their shapes and sizes is crucial. While the technology, namely telescopes, for imaging distant galaxies improves, the amount of data increases exponentially. In the past, these kinds of classifications were done by manual review of experienced astronomers. However, as the datasets grow to the scale of hundreds of millions of galaxies, manual classification becomes impossible to sustain. Thus, this project aims to analyze images of galaxies and to train a neural network to classify them into their distinct morphology classes.</p>
<h2><a href="#project-background" id="project-background" class="header-link">Project Background</a></h2><p>This project is based on the <a href="https://www.zooniverse.org/projects/zookeeper/galaxy-zoo/" target="_blank" rel="noopener noreferrer" class="link_underline__J1V8J">Galaxy Zoo</a> <a href="https://www.kaggle.com/c/galaxy-zoo-the-galaxy-challenge" target="_blank" rel="noopener noreferrer" class="link_underline__J1V8J">Challenge</a>, a crowdsourced effort, through which thousands of volunteers classified galaxies based on a rather simple decision tree, which I'll explain shortly. The provided dataset consists of 61,578 images with corresponding labels.<br>The labels themselves represent a decision tree with 37 classes of the format "ClassA.B", where A represents the level in the decision tree (from 1 to 11) and B represents the choices at a given level. This is quite different to most classification problems, as each class represents an attribute, and most galaxy images will contain multiple attributes. The values in each row of features then represents the confidence of the participants that a given answer is correct, from zero to one.</p>
<p>The decision tree described in the paper is as follows:</p>
<p><img src="./files/decision-tree.png" alt="decisiontree"></p>
<p>The decision process traverses forwards and backwards through the list of questions, with each questions and answer combination representing a class in the labels. The decision tree must always terminate with an END class, which includes classes like 1.3 (star or artifact).</p>
<p>This resulted in the following probability distributions for all 61,578 galaxies</p>
<p><img src="./files/labels.png" alt="labels"></p>
<p>With the general problem defined , it is now time to think about our approach.<br>My objective is the same as the evaluation metric proposed by the Galaxy Zoo Challenge, optimize the element-wise <strong>root-mean-square error</strong> (RMSE) or deviation.<br>Which is defined as:<br><img src="./files/rmse_smoller.png" alt="rmse"></p>
<ul>
<li><strong>N</strong>   is the number of galaxies times the total number of responses</li><li><strong>p</strong> is your predicted value</li><li><strong>a</strong> is the actual value</li></ul>
<p>This will give us a deviation between our predicted values and the actual values. That being said, what's a good RMSE?<br>Well, like with many evaluation metrics in regression problems, it all depends on the domain of your problem. You can't really compare RMSE scores unless they are derived from the same regression problem. I will go into further details on this in the section about benchmarking, including why this obvious classification problem is actually about regression.</p>
<h2><a href="#exploratory-data-analysis" id="exploratory-data-analysis" class="header-link">Exploratory Data Analysis</a></h2><p>The objective of my exploratory data analysis was to get a better understanding of the dataset, I wanted to learn about how the classes are distributed, as well as how the galaxies are displayed in the images, to then better inform my preprocessing and deep learning strategies. For a start, this is what the highest confidence example for each feature looks like. Keep in mind that the classes are still representing a decision tree.</p>
<p><img src="./files/dataset.png" alt="fasd"></p>
<p><img src="./files/confidence-levels.png" alt="mean"></p>
<p>The first thing I looked at was the instances of all classes where the confidence level was above 0.5. This is interesting to me as it will show the classes which are common and which are sparse in the dataset, and my main questions regarding this was whether I needed to apply weights to the loss function. If the classes were all evenly distributed, I could use an out of the box loss function. If not, I would need to write one that applies weights to prevent the neural network from over-prioritizing classes where there isn’t enough data to effectively learn.</p>
<p><img src="./files/above-05.png" alt="above"></p>
<p>As you can see, the classes</p>
<ul>
<li>  6.2 (No odd feature)</li><li>  1.2 (Featured or disc)</li><li>  2.2 (Not edge on)</li><li>  1.1 (Smooth)</li></ul>
<p>are dominating the dataset, which makes sense, as most galaxies in the night sky would share atleast one or more features from this list.<br>We can also see that stars and artifacts (Class 1.3) are extremely rare in this dataset with only 44 instances; pretty much 0%. This means that we won't have to build a model that filters out these anomalies.<br>Following that I wanted to find out how strongly the participants agreed with each other. As we don’t have a single source of truth, we’re not really modeling which galaxies have which features but we are actually measuring how confident the human scorers is that a galaxy has a certain attribute.<br>To understand this better, I plotted correlation matrices for the answers to each question as heatmaps.</p>
<p><img src="./files/corr1.png" alt="corr1"></p>
<p>This rather overwhelming heatmap gives us an overview of the correlations between all the 37 morphology classes. We can observe that the relatively rare Class1.3 (stars or other non-galaxies) is correlated with the path through Class1.1 to ambiguous shapes and odd attribtes. Labelers may have confused galaxies with odd appearances and stars. Things like this are a reminder that hand-labeled data does not represent the absolute truth, but rather an approximation.<br>We can also see from the negative correlation between Class6.1 and Class6.2 that disagreement over whether an images had an odd feature was rare. Although there is high correlation between Class8.3/4/6 , which means that there was some confusion between disturbed, irregular, and merged galaxies.</p>
<p>Zooming into the individual questions shows us how well humans can discern between different attributes.</p>
<p><img src="./files/corr2.png" alt="corr2"></p>
<p>Class6.n shows us the ideal case where it is very unlikely for people to disagree over the galaxies with odd features, while Class9.n reveals that there is regularly confusion over the shape of the bulges and even over whether or not a bulge is present.</p>
<h2><a href="#benchmarking" id="benchmarking" class="header-link">Benchmarking</a></h2><p>Although at heart, this is a classification problem, the objective of modeling the confidence level of humans classifcation transforms it into a regression problem.<br>As mentioned before, I’m going to be using an element-wise root mean-squared-error (RMSE) scoring function. Unlike classification problems where things like precision, confusion matrices, recall, and F-scores are easy to understand, RMSE is a bit more abstract. You can’t really say what a "good" RMSE score is, rather you have to set a baseline benchmark for the minimum possible performance and improve from there.</p>
<p>Provived with the dataset is a “central pixel benchmark” which achieved a RMSE of 0.16194. This is found by predicting the classes simply based on the average scores for images with the same central pixel. Looking at the Galaxy Zoo Challenge leaderboard most participants fall somewhere between 0.10 and 0.12 with the winner achieving a RMSE of just 0.07941.</p>
<p>But since this is a learning exercise, I decided to make my own little benchmark. I chose three strategies:</p>
<ul>
<li>Linear regression based on the central pixels</li><li>Linear regression based on the average pixels</li><li>Average confidences provided by the human classification</li></ul>
<p>The results?</p>
<p>Central Pixel RMSE: <strong>0.1577</strong><br>Average Pixel RMSE: <strong>0.1597</strong><br>Both are better than the provided benchmark! But does this mean anything?</p>
<p>Just the Average Confidence RMSE: <strong>0.1639</strong><br>Linear regression only just beats the prediction by the average confidences.<br>Looks like we’ll have to use deep learning after all!</p>
<h2><a href="#preprocessing" id="preprocessing" class="header-link">Preprocessing</a></h2><p>Each image is of 424x424 pixels in size and comes in color (3 channels).<br>By examining the dataset, most images had large amount of black margins around the area of interest. Because of this, I decided to write a little region of interest algorithm which detects the most prominent part of the image and then crops each individual image to 150x150 pixels around its region of interest.<br>I also converted the images to grayscale, not only in order to reduce computational costs but mainly because the colored images gave me worst results while training.</p>
<p><img src="./files/preprocessing_crop.png" alt="wide"></p>
<p>At first, this worked great, but after playing a bunch with the crop factor while monitoring the model performance, I decided on doing a wider crop to allow for full rotation (this will come in handy for data augmentation).</p>
<p><img src="./files/preprocessing_margin.png" alt="margin"></p>
<h2><a href="#convolutional-neural-network" id="convolutional-neural-network" class="header-link">Convolutional Neural Network</a></h2><p>I chose to use a convolutional neural network (CNN) for this problem. CNNs are uniquely suited for image classification problems because they can explore local relationships between pixels, through a process we know as <a href="#convolutional-layers" class="link_underline__J1V8J">convolution</a>, rather than looking at the whole picture all at once. Their ability to detect features in even large images makes them superior to standard feed forward and recurrent neural networks.</p>
<h3><a href="#architecture" id="architecture" class="header-link">Architecture</a></h3><p>The network architecture I came up with consists of 6 consecutive convolutional layers, each being activated by <a href="https://en.wikipedia.org/wiki/Rectifier_(neural_networks)" target="_blank" rel="noopener noreferrer" class="link_underline__J1V8J">ReLU</a> and followed by <a href="#pooling" class="link_underline__J1V8J">max-pooling</a> and <a href="https://en.wikipedia.org/wiki/Batch_normalization" target="_blank" rel="noopener noreferrer" class="link_underline__J1V8J">batch normalization</a>. Towards the end of the network I introducted <a href="https://en.wikipedia.org/wiki/Dilution_(neural_networks)" target="_blank" rel="noopener noreferrer" class="link_underline__J1V8J">dropout layers</a>, which in essence just randomly kick out neurons at each step during training, which helps prevent overfitting. After that we have a flatten layer to reduce dimensions for the following fully-connected layers. I chose a set of 3 dense layers, with two of the three being again activiated  with ReLU and the last one for the final prediction being activated by a <a href="https://en.wikipedia.org/wiki/Sigmoid_function" target="_blank" rel="noopener noreferrer" class="link_underline__J1V8J">sigmoid function</a>. This last dense layer brings us then to our desired shape of 1x37, for the 37 different morphology classes.</p>
<pre><code class="prism-code language-python"><div class="token-line"><span class="token plain">layers</span><span class="token punctuation">.</span><span class="token plain">Conv2D</span><span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">,</span><span class="token plain"> </span><span class="token punctuation">(</span><span class="token number">7</span><span class="token punctuation">,</span><span class="token number">7</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token plain"> activation</span><span class="token operator">=</span><span class="token string">"relu"</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token plain"></span></div><div class="token-line"><span class="token plain">layers</span><span class="token punctuation">.</span><span class="token plain">BatchNormalization</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token plain"></span></div><div class="token-line"><span class="token plain">layers</span><span class="token punctuation">.</span><span class="token plain">MaxPooling2D</span><span class="token punctuation">(</span><span class="token plain">pool_size</span><span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token plain"></span></div><div class="token-line"><span class="token plain">layers</span><span class="token punctuation">.</span><span class="token plain">Conv2D</span><span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">,</span><span class="token plain"> </span><span class="token punctuation">(</span><span class="token number">7</span><span class="token punctuation">,</span><span class="token number">7</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token plain"> activation</span><span class="token operator">=</span><span class="token string">"relu"</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token plain"></span></div><div class="token-line"><span class="token plain">layers</span><span class="token punctuation">.</span><span class="token plain">BatchNormalization</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token plain"></span></div><div class="token-line"><span class="token plain">layers</span><span class="token punctuation">.</span><span class="token plain">MaxPooling2D</span><span class="token punctuation">(</span><span class="token plain">pool_size</span><span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token plain"></span></div><div class="token-line"><span class="token plain">layers</span><span class="token punctuation">.</span><span class="token plain">Conv2D</span><span class="token punctuation">(</span><span class="token number">64</span><span class="token punctuation">,</span><span class="token plain"> </span><span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">,</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token plain"> activation</span><span class="token operator">=</span><span class="token string">"relu"</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token plain"></span></div><div class="token-line"><span class="token plain">layers</span><span class="token punctuation">.</span><span class="token plain">BatchNormalization</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token plain"></span></div><div class="token-line"><span class="token plain">layers</span><span class="token punctuation">.</span><span class="token plain">MaxPooling2D</span><span class="token punctuation">(</span><span class="token plain">pool_size</span><span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token plain"></span></div><div class="token-line"><span class="token plain">layers</span><span class="token punctuation">.</span><span class="token plain">Conv2D</span><span class="token punctuation">(</span><span class="token number">64</span><span class="token punctuation">,</span><span class="token plain"> </span><span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">,</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token plain"> activation</span><span class="token operator">=</span><span class="token string">"relu"</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token plain"></span></div><div class="token-line"><span class="token plain">layers</span><span class="token punctuation">.</span><span class="token plain">BatchNormalization</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token plain"></span></div><div class="token-line"><span class="token plain">layers</span><span class="token punctuation">.</span><span class="token plain">MaxPooling2D</span><span class="token punctuation">(</span><span class="token plain">pool_size</span><span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token plain"></span></div><div class="token-line"><span class="token plain">layers</span><span class="token punctuation">.</span><span class="token plain">Conv2D</span><span class="token punctuation">(</span><span class="token number">128</span><span class="token punctuation">,</span><span class="token plain"> </span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token plain"> activation</span><span class="token operator">=</span><span class="token string">"relu"</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token plain"></span></div><div class="token-line"><span class="token plain">layers</span><span class="token punctuation">.</span><span class="token plain">BatchNormalization</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token plain"></span></div><div class="token-line"><span class="token plain">layers</span><span class="token punctuation">.</span><span class="token plain">MaxPooling2D</span><span class="token punctuation">(</span><span class="token plain">pool_size</span><span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token plain"></span></div><div class="token-line"><span class="token plain">layers</span><span class="token punctuation">.</span><span class="token plain">Conv2D</span><span class="token punctuation">(</span><span class="token number">256</span><span class="token punctuation">,</span><span class="token plain"> </span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token plain"> activation</span><span class="token operator">=</span><span class="token string">"relu"</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token plain"></span></div><div class="token-line"><span class="token plain">layers</span><span class="token punctuation">.</span><span class="token plain">BatchNormalization</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token plain"></span></div><div class="token-line"><span class="token plain">layers</span><span class="token punctuation">.</span><span class="token plain">MaxPooling2D</span><span class="token punctuation">(</span><span class="token plain">pool_size</span><span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token plain"></span></div><div class="token-line"><span class="token plain">layers</span><span class="token punctuation">.</span><span class="token plain">Dropout</span><span class="token punctuation">(</span><span class="token number">0.05</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token plain"></span></div><div class="token-line"><span class="token plain">layers</span><span class="token punctuation">.</span><span class="token plain">Flatten</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token plain"></span></div><div class="token-line"><span class="token plain">layers</span><span class="token punctuation">.</span><span class="token plain">Dense</span><span class="token punctuation">(</span><span class="token number">512</span><span class="token punctuation">,</span><span class="token plain"> activation</span><span class="token operator">=</span><span class="token string">"relu"</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token plain"></span></div><div class="token-line"><span class="token plain">layers</span><span class="token punctuation">.</span><span class="token plain">Droput</span><span class="token punctuation">(</span><span class="token number">0.15</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token plain"></span></div><div class="token-line"><span class="token plain">layers</span><span class="token punctuation">.</span><span class="token plain">Dense</span><span class="token punctuation">(</span><span class="token number">256</span><span class="token punctuation">,</span><span class="token plain"> activation</span><span class="token operator">=</span><span class="token string">"relu"</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token plain"></span></div><div class="token-line"><span class="token plain">layers</span><span class="token punctuation">.</span><span class="token plain">Dropout</span><span class="token punctuation">(</span><span class="token number">0.15</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token plain"></span></div><div class="token-line"><span class="token plain">layers</span><span class="token punctuation">.</span><span class="token plain">Dense</span><span class="token punctuation">(</span><span class="token number">37</span><span class="token punctuation">,</span><span class="token plain"> activation</span><span class="token operator">=</span><span class="token string">"sigmoid"</span><span class="token punctuation">)</span></div></code></pre><p>This configuration resulted in <strong>848,133</strong> total trainable parameters.</p>
<h3><a href="#convolutional-layers" id="convolutional-layers" class="header-link">Convolutional Layers</a></h3><p>Convolutional layers are composed of kernels, or <a href="https://en.wikipedia.org/wiki/Kernel_(image_processing)" target="_blank" rel="noopener noreferrer" class="link_underline__J1V8J">image filters</a>. Filters pass over an image in steps, creating a new image where each pixel is the convolution of the filter matrix and a subsection of the image. <a href="https://en.wikipedia.org/wiki/Convolution" target="_blank" rel="noopener noreferrer" class="link_underline__J1V8J">Convolution</a> is a matrix operation that is in essence a weighted combination of two matrices. The result of this operation is called a <strong>feature map</strong>. The feature map is essentially the output of one filter applied to the previous layer.<br><img src="./files/featuremap.png" alt="featuremap"> <em>Example of a feature map for recognising handwritten digits</em></p>
<p>A given filter is drawn across the entire previous layer, moved one pixel at a time, with each position resulting in an activation of a neuron and the output being collected in the feature map. These filters allow the network to analyze local relationships between pixels rather than looking at the entire image. One typical filter is a sharpening filter, in 3x3 form here, where <strong>n</strong> is the level of sharpening to be applied.</p>
<p>  0   -1   0</p>
<p>  -1   n   -1</p>
<p>  0   -1   0</p>
<p>Anyone familiar with image processing will recognize this kernel. What makes a CNN powerful is that the convolutional layers are built from neurons themselves which will in effect learn new filters that are uniquely fitted to identifying and understanding features in the images. It will likely learn an edge detection filter and other standard filters, but it will also, as the network depth increases, learn unique filters that pick up subtle details.</p>
<h3><a href="#pooling" id="pooling" class="header-link">Pooling</a></h3><p>After each convolutional layer, we are increasing the width of our network, that is the number of filtered images. As we add on layers, we begin to suffer from the sheer dimensionality of the network, with the number of neurons skyrocketing. This becomes increasingly computationally expensive, and keeping that extra data hurts actually more than it helps. Because our objective with the learned kernels is to recognize patterns, reducing the size of the matrices effectively allows the neural network to zoom out and look at relationships between features recognized at previous convolution steps.<br>Remember that our ultimate goal is to understand the relationship between the pixels in an image and turn that information into 37 answers to 11 questions. As information flows through the network, higher level features are being extracted and it is at this point that it is important to recognize a wider variety of features without getting bogged down in tiny details. This is where Pooling comes in. Pooling layers decrease the size of our matrices by breaking them down into grids, say 2x2, and reducing them each to a single pixel. There are a few strategies, like max, sum, average, etc, with each having their own strenghts and benefits. I chose to use max pooling, which kind of performs like a sharpening filter, by just taking the maximum value for each patch of the feature map.</p>
<p><img src="./files/pooling.svg" alt="asd"></p>
<h3><a href="#data-augmentation" id="data-augmentation" class="header-link">Data Augmentation</a></h3><p>Data augmentation is a popular technique to artificially increase the diversity of your training set by applying random (but realistic) transformations, such as image rotation, scaling and flipping. I decided to use the <a href="https://www.tensorflow.org/tutorials/images/data_augmentation" target="_blank" rel="noopener noreferrer" class="link_underline__J1V8J">preprocessing layers</a> built into TensorFlow to achieve this. Their method doesn't actually increase the size of the dataset like you would expect, but applies the aforementioned transformations in between training. I chose this method over creating completely new images, as my dataset is already large enough.</p>
<pre><code class="prism-code language-python"><div class="token-line"><span class="token plain">layers</span><span class="token punctuation">.</span><span class="token plain">preprocessing</span><span class="token punctuation">.</span><span class="token plain">RandomFlip</span><span class="token punctuation">(</span><span class="token string">"horizontal_and_vertical"</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token plain"></span></div><div class="token-line"><span class="token plain">layers</span><span class="token punctuation">.</span><span class="token plain">preprocessing</span><span class="token punctuation">.</span><span class="token plain">RandomRotation</span><span class="token punctuation">(</span><span class="token number">0.3</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token plain"></span></div><div class="token-line"><span class="token plain">layers</span><span class="token punctuation">.</span><span class="token plain">preprocessing</span><span class="token punctuation">.</span><span class="token plain">RandomZoom</span><span class="token punctuation">(</span><span class="token plain">height_factor</span><span class="token operator">=</span><span class="token number">0.2</span><span class="token punctuation">,</span><span class="token plain"> width_factor</span><span class="token operator">=</span><span class="token number">0.2</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token plain"></span></div><div class="token-line"><span class="token plain">layers</span><span class="token punctuation">.</span><span class="token plain">preprocessing</span><span class="token punctuation">.</span><span class="token plain">RandomContrast</span><span class="token punctuation">(</span><span class="token number">0.05</span><span class="token punctuation">)</span><span class="token punctuation">,</span></div></code></pre><h3><a href="#training" id="training" class="header-link">Training</a></h3><p>After some consideration I settled on training my CNN with the Adam optimizer. You could think of Adam as an extension to classical stochastic gradient descent. While traditional SGD maintains a single learning rate for all weight updates and does not adapt during training, Adam and other variants of SGD <strong>can</strong> adjust the learning rate on the fly during training, which lets them achieve much faster convergence speeds than vanilla SGD. I still don't really know how to optimize their parameters and certainly can't appreciate how they work on a deeper mathematical level, but that is beyond the scope of this project. All you really have to know is that Adam is a little bit faster than stochastic gradient descent with comparable performance. Training the full model over 50 epochs took about 7 hours on a GPU-accelerated workstation (RTX 3080 Ti)</p>
<p><img src="./files/finaltrain.png" alt="finaltrain"></p>
<p>As you can see in both the RMSE over epoch and loss over epoch, the model converges pretty well. Although I initially had trouble with the model overfitting after around 25 epochs. I solved this by increasing the depth of the network from the previous 3 consecutive convolutional layers to 6. I also saw major improvements after applying data augmentation, which helped the model quite a bit to increase its accuracy after epoch 25. Another key improvement came after finding the right combination of kernel and filter sizes. As it turn out CNNs with multiple consecutive convolutional layers learn best with <strong>increasing filter</strong> and <strong>decreasing kernel sizes</strong>. This whole process of finding the right combination of hyperparameters, felt a lot like <a href="https://xkcd.com/1838/" target="_blank" rel="noopener noreferrer" class="link_underline__J1V8J">stiring the pile</a> until it starts looking right. I was initially going to do an empirical approach to optimizing my model, meaning I would write a program that creates a bunch of models with different hyperparametes, trains them for a couple of epochs and then evaluates which combination yielded the best results. But due to time constraints I decided to just go with trial and error. Perhaps this is something I am going to revisit the next time I will build a neural network.</p>
<h2><a href="#results" id="results" class="header-link">Results</a></h2><p>My final result? A respectable score of <strong>0.0926</strong>! I would have placed 28th in the original <a href="https://www.kaggle.com/c/galaxy-zoo-the-galaxy-challenge/leaderboard" target="_blank" rel="noopener noreferrer" class="link_underline__J1V8J">Galaxy Zoo competition</a>. I am more than happy with my final score, as I had limited time to work on this project. Again comparing it to the other methods, the convolutional neural network yielded the lowest deviation. </p>
<table>
<thead>
<tr>
<th>Method</th>
<th></th>
<th></th>
<th>RMSE</th>
</tr>
</thead>
<tbody><tr>
<td>LR (central pixel)</td>
<td></td>
<td></td>
<td>0.1577</td>
</tr>
<tr>
<td>LR (average pixel)</td>
<td></td>
<td></td>
<td>0.1597</td>
</tr>
<tr>
<td>CNN</td>
<td></td>
<td></td>
<td>0.0926</td>
</tr>
</tbody></table>
<p>CNNs are once again proving to be a good machine learning method for finding patterns in image data.</p>
<h2><a href="#installation" id="installation" class="header-link">Installation</a></h2><p>Download the source code</p>
<pre><code class="prism-code language-bash"><div class="token-line"><span class="token plain">$ </span><span class="token function">git</span><span class="token plain"> clone https://github.com/paulphys/galaxyclass</span></div></code></pre><p>The source code already includes instructions for downloading the dataset, but in case you want to get it separately</p>
<pre><code class="prism-code language-bash"><div class="token-line"><span class="token plain">$ </span><span class="token function">curl</span><span class="token plain"> -LO https://physics.sh/galaxyclass-data.zip</span></div></code></pre><h2><a href="#usage" id="usage" class="header-link">Usage</a></h2><p>This is what the file structure of the provided repository looks like.</p>
<p><img src="./files/filetree.png" alt="filetree"></p>
<p>All of the code for this project lives in the <code>code/galaxyclass.ipynb</code> Jupyter Notebook, so make sure you have <a href="https://jupyter.org/install" target="_blank" rel="noopener noreferrer" class="link_underline__J1V8J">Jupyter</a> installed before proceeding. Inside of <code>data/models/</code> you can find my final trained model and the folder <code>report/</code> contains this document and the HTML version of the notebook.</p>
<h3><a href="#jupyter-notebook" id="jupyter-notebook" class="header-link">Jupyter Notebook</a></h3><p>Start the notebook</p>
<pre><code class="prism-code language-bash"><div class="token-line"><span class="token plain">$ jupyter-notebook galaxyclass.ipynb</span></div></code></pre><p>From here you just have to execute all cells going from top to bottom, starting with the installation of the dependencies and hardware initialization. If you plan to train the model for yourself, I highly recommend using GPU acceleration, otherwise it is going to take ages to train on the CPU. For that I also included two methods for allocating GPU memory, with the latter restricting access to a predefined amount of memory, in case you want/have to share resources between multiple users or processes.</p>
<h2><a href="#conclusion" id="conclusion" class="header-link">Conclusion</a></h2><p>I had plenty of fun working on this project. This definitely sparked my interests going into computational physics and I feel very grateful to have had this opportunity. Given more time I would experiment more with both the preprocessing, particularly with the region of interest detection cropping off galaxies at the far edge, as well as exploring other network architectures, like <strong>ResNet</strong>, which have been shown to outperform traditional CNNs. To further improve this model I could also try to implement feature extraction to train the model only on selected features rather than all pixels at once.<br>All in all, I gained some interesting insights into the computational methods involved in physics research and I am looking forward to doing more of it in the future.</p>
<h2><a href="#references" id="references" class="header-link">References</a></h2><p>[<strong>1</strong>] Galaxy Zoo : Morphologies derived from visual inspection of galaxies from the Sloan Digital Sky Survey<br><a href="https://arxiv.org/abs/0804.4483" target="_blank" rel="noopener noreferrer" class="link_underline__J1V8J">https://arxiv.org/abs/0804.4483</a><br>[<strong>2</strong>] Galaxy Zoo 2: detailed morphological classifications for 304,122 galaxies from the Sloan Digital Sky Survey<br> <a href="https://arxiv.org/abs/1308.3496" target="_blank" rel="noopener noreferrer" class="link_underline__J1V8J">https://arxiv.org/abs/1308.3496</a><br>[<strong>3</strong>] Galaxy Zoo - The Galaxy Challenge<br><a href="https://www.kaggle.com/c/galaxy-zoo-the-galaxy-challenge/" target="_blank" rel="noopener noreferrer" class="link_underline__J1V8J">https://www.kaggle.com/c/galaxy-zoo-the-galaxy-challenge/</a><br>[<strong>4</strong>] Maximum Pooling and Average Pooling<br><a href="https://d2l.ai/chapter_convolutional-neural-networks/pooling.html" target="_blank" rel="noopener noreferrer" class="link_underline__J1V8J">https://d2l.ai/chapter_convolutional-neural-networks/pooling.html</a><br>[<strong>5</strong>] Visualizing Feature Maps and Filters<br><a href="https://medium.com/dataseries/visualizing-the-feature-maps-and-filters-by-convolutional-neural-networks-e1462340518e" target="_blank" rel="noopener noreferrer" class="link_underline__J1V8J">https://medium.com/dataseries/visualizing-the-feature-maps-and-filters-by-convolutional-neural-networks-e1462340518e</a></p>
</article><div class="navigation_navigation__CfrCb"><div class="navigation_previous__aaYm2"></div><div class="navigation_next__UDjD0"></div></div></main></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"previous":null,"next":null,"title":"Galaxy morphology classification with Convolutional Neural Networks","description":"Galaxy morphology classification with Convolutional Neural Networks","slug":"galaxyclass","date":"astrophysics | deep learning","html":"\u003cp\u003e\u003cimg src=\"/galaxyclass/galaxyclass-header.png\" alt=\"galaxyclass\"\u003e\u003c/p\u003e\n\u003cp\u003eThis is a deep learning project that I completed for \u003ca href=https://www.ph.tum.de/academics/org/cc/mh/PH0101/ target=\"_blank\" rel=\"noopener noreferrer\" class=\"link_underline__J1V8J\"\u003ePH0101\u003c/a\u003e as part of my undergraduate in physics. I’m going to walk you through the entire workflow for this project, including exploratory data analysis, image preprocessing, and finally building a convolutional neural network (CNN) with TensorFlow and the Keras API.\u003c/p\u003e\n\u003cp\u003e\u003ca href=/galaxyclass/notebook.html target=\"_blank\" rel=\"noopener noreferrer\" class=\"link_underline__J1V8J\"\u003eView the Jupyter Notebook (HTML version)\u003c/a\u003e, which contains the full source code as well as my learning notes.\u003c/p\u003e\n\u003ch3\u003e\u003ca href=\"#index\" id=\"index\" class=\"header-link\"\u003eIndex\u003c/a\u003e\u003c/h3\u003e\u003col\u003e\n\u003cli\u003e\u003ca href=#motivation target=\"_blank\" rel=\"noopener noreferrer\" class=\"link_underline__J1V8J\"\u003eMotivation\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=#project-background target=\"_blank\" rel=\"noopener noreferrer\" class=\"link_underline__J1V8J\"\u003eProject Background\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=#exploratory-data-analysis target=\"_blank\" rel=\"noopener noreferrer\" class=\"link_underline__J1V8J\"\u003eExploratory Data Analysis\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=#benchmarking target=\"_blank\" rel=\"noopener noreferrer\" class=\"link_underline__J1V8J\"\u003eBenchmarking\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=#preprocessing target=\"_blank\" rel=\"noopener noreferrer\" class=\"link_underline__J1V8J\"\u003ePreprocessing\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=#convolutional-neural-network target=\"_blank\" rel=\"noopener noreferrer\" class=\"link_underline__J1V8J\"\u003eConvolutional Neural Network\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=#results target=\"_blank\" rel=\"noopener noreferrer\" class=\"link_underline__J1V8J\"\u003eResults\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=#dataset target=\"_blank\" rel=\"noopener noreferrer\" class=\"link_underline__J1V8J\"\u003eInstallation\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=#dataset target=\"_blank\" rel=\"noopener noreferrer\" class=\"link_underline__J1V8J\"\u003eUsage\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=#conclusion target=\"_blank\" rel=\"noopener noreferrer\" class=\"link_underline__J1V8J\"\u003eConclusion\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=#references target=\"_blank\" rel=\"noopener noreferrer\" class=\"link_underline__J1V8J\"\u003eReferences\u003c/a\u003e\u003c/li\u003e\u003c/ol\u003e\n\u003ch2\u003e\u003ca href=\"#motivation\" id=\"motivation\" class=\"header-link\"\u003eMotivation\u003c/a\u003e\u003c/h2\u003e\u003cp\u003eAs the universe continues to expand, questions remain about how galaxies formed and evolved. To solve this puzzle of cosmic structure, understanding the distribution and types of galaxies as a function of their shapes and sizes is crucial. While the technology, namely telescopes, for imaging distant galaxies improves, the amount of data increases exponentially. In the past, these kinds of classifications were done by manual review of experienced astronomers. However, as the datasets grow to the scale of hundreds of millions of galaxies, manual classification becomes impossible to sustain. Thus, this project aims to analyze images of galaxies and to train a neural network to classify them into their distinct morphology classes.\u003c/p\u003e\n\u003ch2\u003e\u003ca href=\"#project-background\" id=\"project-background\" class=\"header-link\"\u003eProject Background\u003c/a\u003e\u003c/h2\u003e\u003cp\u003eThis project is based on the \u003ca href=https://www.zooniverse.org/projects/zookeeper/galaxy-zoo/ target=\"_blank\" rel=\"noopener noreferrer\" class=\"link_underline__J1V8J\"\u003eGalaxy Zoo\u003c/a\u003e \u003ca href=https://www.kaggle.com/c/galaxy-zoo-the-galaxy-challenge target=\"_blank\" rel=\"noopener noreferrer\" class=\"link_underline__J1V8J\"\u003eChallenge\u003c/a\u003e, a crowdsourced effort, through which thousands of volunteers classified galaxies based on a rather simple decision tree, which I\u0026#39;ll explain shortly. The provided dataset consists of 61,578 images with corresponding labels.\u003cbr\u003eThe labels themselves represent a decision tree with 37 classes of the format \u0026quot;ClassA.B\u0026quot;, where A represents the level in the decision tree (from 1 to 11) and B represents the choices at a given level. This is quite different to most classification problems, as each class represents an attribute, and most galaxy images will contain multiple attributes. The values in each row of features then represents the confidence of the participants that a given answer is correct, from zero to one.\u003c/p\u003e\n\u003cp\u003eThe decision tree described in the paper is as follows:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/galaxyclass/decision-tree.png\" alt=\"decisiontree\"\u003e\u003c/p\u003e\n\u003cp\u003eThe decision process traverses forwards and backwards through the list of questions, with each questions and answer combination representing a class in the labels. The decision tree must always terminate with an END class, which includes classes like 1.3 (star or artifact).\u003c/p\u003e\n\u003cp\u003eThis resulted in the following probability distributions for all 61,578 galaxies\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/galaxyclass/labels.png\" alt=\"labels\"\u003e\u003c/p\u003e\n\u003cp\u003eWith the general problem defined , it is now time to think about our approach.\u003cbr\u003eMy objective is the same as the evaluation metric proposed by the Galaxy Zoo Challenge, optimize the element-wise \u003cstrong\u003eroot-mean-square error\u003c/strong\u003e (RMSE) or deviation.\u003cbr\u003eWhich is defined as:\u003cbr\u003e\u003cimg src=\"/galaxyclass/rmse_smoller.png\" alt=\"rmse\"\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eN\u003c/strong\u003e   is the number of galaxies times the total number of responses\u003c/li\u003e\u003cli\u003e\u003cstrong\u003ep\u003c/strong\u003e is your predicted value\u003c/li\u003e\u003cli\u003e\u003cstrong\u003ea\u003c/strong\u003e is the actual value\u003c/li\u003e\u003c/ul\u003e\n\u003cp\u003eThis will give us a deviation between our predicted values and the actual values. That being said, what\u0026#39;s a good RMSE?\u003cbr\u003eWell, like with many evaluation metrics in regression problems, it all depends on the domain of your problem. You can\u0026#39;t really compare RMSE scores unless they are derived from the same regression problem. I will go into further details on this in the section about benchmarking, including why this obvious classification problem is actually about regression.\u003c/p\u003e\n\u003ch2\u003e\u003ca href=\"#exploratory-data-analysis\" id=\"exploratory-data-analysis\" class=\"header-link\"\u003eExploratory Data Analysis\u003c/a\u003e\u003c/h2\u003e\u003cp\u003eThe objective of my exploratory data analysis was to get a better understanding of the dataset, I wanted to learn about how the classes are distributed, as well as how the galaxies are displayed in the images, to then better inform my preprocessing and deep learning strategies. For a start, this is what the highest confidence example for each feature looks like. Keep in mind that the classes are still representing a decision tree.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/galaxyclass/dataset.png\" alt=\"fasd\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/galaxyclass/confidence-levels.png\" alt=\"mean\"\u003e\u003c/p\u003e\n\u003cp\u003eThe first thing I looked at was the instances of all classes where the confidence level was above 0.5. This is interesting to me as it will show the classes which are common and which are sparse in the dataset, and my main questions regarding this was whether I needed to apply weights to the loss function. If the classes were all evenly distributed, I could use an out of the box loss function. If not, I would need to write one that applies weights to prevent the neural network from over-prioritizing classes where there isn’t enough data to effectively learn.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/galaxyclass/above-05.png\" alt=\"above\"\u003e\u003c/p\u003e\n\u003cp\u003eAs you can see, the classes\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e  6.2 (No odd feature)\u003c/li\u003e\u003cli\u003e  1.2 (Featured or disc)\u003c/li\u003e\u003cli\u003e  2.2 (Not edge on)\u003c/li\u003e\u003cli\u003e  1.1 (Smooth)\u003c/li\u003e\u003c/ul\u003e\n\u003cp\u003eare dominating the dataset, which makes sense, as most galaxies in the night sky would share atleast one or more features from this list.\u003cbr\u003eWe can also see that stars and artifacts (Class 1.3) are extremely rare in this dataset with only 44 instances; pretty much 0%. This means that we won\u0026#39;t have to build a model that filters out these anomalies.\u003cbr\u003eFollowing that I wanted to find out how strongly the participants agreed with each other. As we don’t have a single source of truth, we’re not really modeling which galaxies have which features but we are actually measuring how confident the human scorers is that a galaxy has a certain attribute.\u003cbr\u003eTo understand this better, I plotted correlation matrices for the answers to each question as heatmaps.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/galaxyclass/corr1.png\" alt=\"corr1\"\u003e\u003c/p\u003e\n\u003cp\u003eThis rather overwhelming heatmap gives us an overview of the correlations between all the 37 morphology classes. We can observe that the relatively rare Class1.3 (stars or other non-galaxies) is correlated with the path through Class1.1 to ambiguous shapes and odd attribtes. Labelers may have confused galaxies with odd appearances and stars. Things like this are a reminder that hand-labeled data does not represent the absolute truth, but rather an approximation.\u003cbr\u003eWe can also see from the negative correlation between Class6.1 and Class6.2 that disagreement over whether an images had an odd feature was rare. Although there is high correlation between Class8.3/4/6 , which means that there was some confusion between disturbed, irregular, and merged galaxies.\u003c/p\u003e\n\u003cp\u003eZooming into the individual questions shows us how well humans can discern between different attributes.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/galaxyclass/corr2.png\" alt=\"corr2\"\u003e\u003c/p\u003e\n\u003cp\u003eClass6.n shows us the ideal case where it is very unlikely for people to disagree over the galaxies with odd features, while Class9.n reveals that there is regularly confusion over the shape of the bulges and even over whether or not a bulge is present.\u003c/p\u003e\n\u003ch2\u003e\u003ca href=\"#benchmarking\" id=\"benchmarking\" class=\"header-link\"\u003eBenchmarking\u003c/a\u003e\u003c/h2\u003e\u003cp\u003eAlthough at heart, this is a classification problem, the objective of modeling the confidence level of humans classifcation transforms it into a regression problem.\u003cbr\u003eAs mentioned before, I’m going to be using an element-wise root mean-squared-error (RMSE) scoring function. Unlike classification problems where things like precision, confusion matrices, recall, and F-scores are easy to understand, RMSE is a bit more abstract. You can’t really say what a \u0026quot;good\u0026quot; RMSE score is, rather you have to set a baseline benchmark for the minimum possible performance and improve from there.\u003c/p\u003e\n\u003cp\u003eProvived with the dataset is a “central pixel benchmark” which achieved a RMSE of 0.16194. This is found by predicting the classes simply based on the average scores for images with the same central pixel. Looking at the Galaxy Zoo Challenge leaderboard most participants fall somewhere between 0.10 and 0.12 with the winner achieving a RMSE of just 0.07941.\u003c/p\u003e\n\u003cp\u003eBut since this is a learning exercise, I decided to make my own little benchmark. I chose three strategies:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eLinear regression based on the central pixels\u003c/li\u003e\u003cli\u003eLinear regression based on the average pixels\u003c/li\u003e\u003cli\u003eAverage confidences provided by the human classification\u003c/li\u003e\u003c/ul\u003e\n\u003cp\u003eThe results?\u003c/p\u003e\n\u003cp\u003eCentral Pixel RMSE: \u003cstrong\u003e0.1577\u003c/strong\u003e\u003cbr\u003eAverage Pixel RMSE: \u003cstrong\u003e0.1597\u003c/strong\u003e\u003cbr\u003eBoth are better than the provided benchmark! But does this mean anything?\u003c/p\u003e\n\u003cp\u003eJust the Average Confidence RMSE: \u003cstrong\u003e0.1639\u003c/strong\u003e\u003cbr\u003eLinear regression only just beats the prediction by the average confidences.\u003cbr\u003eLooks like we’ll have to use deep learning after all!\u003c/p\u003e\n\u003ch2\u003e\u003ca href=\"#preprocessing\" id=\"preprocessing\" class=\"header-link\"\u003ePreprocessing\u003c/a\u003e\u003c/h2\u003e\u003cp\u003eEach image is of 424x424 pixels in size and comes in color (3 channels).\u003cbr\u003eBy examining the dataset, most images had large amount of black margins around the area of interest. Because of this, I decided to write a little region of interest algorithm which detects the most prominent part of the image and then crops each individual image to 150x150 pixels around its region of interest.\u003cbr\u003eI also converted the images to grayscale, not only in order to reduce computational costs but mainly because the colored images gave me worst results while training.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/galaxyclass/preprocessing_crop.png\" alt=\"wide\"\u003e\u003c/p\u003e\n\u003cp\u003eAt first, this worked great, but after playing a bunch with the crop factor while monitoring the model performance, I decided on doing a wider crop to allow for full rotation (this will come in handy for data augmentation).\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/galaxyclass/preprocessing_margin.png\" alt=\"margin\"\u003e\u003c/p\u003e\n\u003ch2\u003e\u003ca href=\"#convolutional-neural-network\" id=\"convolutional-neural-network\" class=\"header-link\"\u003eConvolutional Neural Network\u003c/a\u003e\u003c/h2\u003e\u003cp\u003eI chose to use a convolutional neural network (CNN) for this problem. CNNs are uniquely suited for image classification problems because they can explore local relationships between pixels, through a process we know as \u003ca href=#convolutional-layers target=\"_blank\" rel=\"noopener noreferrer\" class=\"link_underline__J1V8J\"\u003econvolution\u003c/a\u003e, rather than looking at the whole picture all at once. Their ability to detect features in even large images makes them superior to standard feed forward and recurrent neural networks.\u003c/p\u003e\n\u003ch3\u003e\u003ca href=\"#architecture\" id=\"architecture\" class=\"header-link\"\u003eArchitecture\u003c/a\u003e\u003c/h3\u003e\u003cp\u003eThe network architecture I came up with consists of 6 consecutive convolutional layers, each being activated by \u003ca href=https://en.wikipedia.org/wiki/Rectifier_(neural_networks) target=\"_blank\" rel=\"noopener noreferrer\" class=\"link_underline__J1V8J\"\u003eReLU\u003c/a\u003e and followed by \u003ca href=#pooling target=\"_blank\" rel=\"noopener noreferrer\" class=\"link_underline__J1V8J\"\u003emax-pooling\u003c/a\u003e and \u003ca href=https://en.wikipedia.org/wiki/Batch_normalization target=\"_blank\" rel=\"noopener noreferrer\" class=\"link_underline__J1V8J\"\u003ebatch normalization\u003c/a\u003e. Towards the end of the network I introducted \u003ca href=https://en.wikipedia.org/wiki/Dilution_(neural_networks) target=\"_blank\" rel=\"noopener noreferrer\" class=\"link_underline__J1V8J\"\u003edropout layers\u003c/a\u003e, which in essence just randomly kick out neurons at each step during training, which helps prevent overfitting. After that we have a flatten layer to reduce dimensions for the following fully-connected layers. I chose a set of 3 dense layers, with two of the three being again activiated  with ReLU and the last one for the final prediction being activated by a \u003ca href=https://en.wikipedia.org/wiki/Sigmoid_function target=\"_blank\" rel=\"noopener noreferrer\" class=\"link_underline__J1V8J\"\u003esigmoid function\u003c/a\u003e. This last dense layer brings us then to our desired shape of 1x37, for the 37 different morphology classes.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"prism-code language-python\"\u003e\u003cdiv class=\"token-line\"\u003e\u003cspan class=\"token plain\"\u003elayers\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e.\u003c/span\u003e\u003cspan class=\"token plain\"\u003eConv2D\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e(\u003c/span\u003e\u003cspan class=\"token number\"\u003e32\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e\u003cspan class=\"token plain\"\u003e \u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e(\u003c/span\u003e\u003cspan class=\"token number\"\u003e7\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e\u003cspan class=\"token number\"\u003e7\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e)\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e\u003cspan class=\"token plain\"\u003e activation\u003c/span\u003e\u003cspan class=\"token operator\"\u003e=\u003c/span\u003e\u003cspan class=\"token string\"\u003e\u0026quot;relu\u0026quot;\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e)\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e\u003cspan class=\"token plain\"\u003e\u003c/span\u003e\u003c/div\u003e\u003cdiv class=\"token-line\"\u003e\u003cspan class=\"token plain\"\u003elayers\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e.\u003c/span\u003e\u003cspan class=\"token plain\"\u003eBatchNormalization\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e(\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e)\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e\u003cspan class=\"token plain\"\u003e\u003c/span\u003e\u003c/div\u003e\u003cdiv class=\"token-line\"\u003e\u003cspan class=\"token plain\"\u003elayers\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e.\u003c/span\u003e\u003cspan class=\"token plain\"\u003eMaxPooling2D\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e(\u003c/span\u003e\u003cspan class=\"token plain\"\u003epool_size\u003c/span\u003e\u003cspan class=\"token operator\"\u003e=\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e(\u003c/span\u003e\u003cspan class=\"token number\"\u003e2\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e\u003cspan class=\"token number\"\u003e2\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e)\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e)\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e\u003cspan class=\"token plain\"\u003e\u003c/span\u003e\u003c/div\u003e\u003cdiv class=\"token-line\"\u003e\u003cspan class=\"token plain\"\u003elayers\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e.\u003c/span\u003e\u003cspan class=\"token plain\"\u003eConv2D\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e(\u003c/span\u003e\u003cspan class=\"token number\"\u003e32\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e\u003cspan class=\"token plain\"\u003e \u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e(\u003c/span\u003e\u003cspan class=\"token number\"\u003e7\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e\u003cspan class=\"token number\"\u003e7\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e)\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e\u003cspan class=\"token plain\"\u003e activation\u003c/span\u003e\u003cspan class=\"token operator\"\u003e=\u003c/span\u003e\u003cspan class=\"token string\"\u003e\u0026quot;relu\u0026quot;\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e)\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e\u003cspan class=\"token plain\"\u003e\u003c/span\u003e\u003c/div\u003e\u003cdiv class=\"token-line\"\u003e\u003cspan class=\"token plain\"\u003elayers\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e.\u003c/span\u003e\u003cspan class=\"token plain\"\u003eBatchNormalization\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e(\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e)\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e\u003cspan class=\"token plain\"\u003e\u003c/span\u003e\u003c/div\u003e\u003cdiv class=\"token-line\"\u003e\u003cspan class=\"token plain\"\u003elayers\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e.\u003c/span\u003e\u003cspan class=\"token plain\"\u003eMaxPooling2D\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e(\u003c/span\u003e\u003cspan class=\"token plain\"\u003epool_size\u003c/span\u003e\u003cspan class=\"token operator\"\u003e=\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e(\u003c/span\u003e\u003cspan class=\"token number\"\u003e2\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e\u003cspan class=\"token number\"\u003e2\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e)\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e)\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e\u003cspan class=\"token plain\"\u003e\u003c/span\u003e\u003c/div\u003e\u003cdiv class=\"token-line\"\u003e\u003cspan class=\"token plain\"\u003elayers\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e.\u003c/span\u003e\u003cspan class=\"token plain\"\u003eConv2D\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e(\u003c/span\u003e\u003cspan class=\"token number\"\u003e64\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e\u003cspan class=\"token plain\"\u003e \u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e(\u003c/span\u003e\u003cspan class=\"token number\"\u003e5\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e\u003cspan class=\"token number\"\u003e5\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e)\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e\u003cspan class=\"token plain\"\u003e activation\u003c/span\u003e\u003cspan class=\"token operator\"\u003e=\u003c/span\u003e\u003cspan class=\"token string\"\u003e\u0026quot;relu\u0026quot;\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e)\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e\u003cspan class=\"token plain\"\u003e\u003c/span\u003e\u003c/div\u003e\u003cdiv class=\"token-line\"\u003e\u003cspan class=\"token plain\"\u003elayers\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e.\u003c/span\u003e\u003cspan class=\"token plain\"\u003eBatchNormalization\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e(\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e)\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e\u003cspan class=\"token plain\"\u003e\u003c/span\u003e\u003c/div\u003e\u003cdiv class=\"token-line\"\u003e\u003cspan class=\"token plain\"\u003elayers\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e.\u003c/span\u003e\u003cspan class=\"token plain\"\u003eMaxPooling2D\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e(\u003c/span\u003e\u003cspan class=\"token plain\"\u003epool_size\u003c/span\u003e\u003cspan class=\"token operator\"\u003e=\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e(\u003c/span\u003e\u003cspan class=\"token number\"\u003e2\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e\u003cspan class=\"token number\"\u003e2\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e)\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e)\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e\u003cspan class=\"token plain\"\u003e\u003c/span\u003e\u003c/div\u003e\u003cdiv class=\"token-line\"\u003e\u003cspan class=\"token plain\"\u003elayers\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e.\u003c/span\u003e\u003cspan class=\"token plain\"\u003eConv2D\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e(\u003c/span\u003e\u003cspan class=\"token number\"\u003e64\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e\u003cspan class=\"token plain\"\u003e \u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e(\u003c/span\u003e\u003cspan class=\"token number\"\u003e5\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e\u003cspan class=\"token number\"\u003e5\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e)\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e\u003cspan class=\"token plain\"\u003e activation\u003c/span\u003e\u003cspan class=\"token operator\"\u003e=\u003c/span\u003e\u003cspan class=\"token string\"\u003e\u0026quot;relu\u0026quot;\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e)\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e\u003cspan class=\"token plain\"\u003e\u003c/span\u003e\u003c/div\u003e\u003cdiv class=\"token-line\"\u003e\u003cspan class=\"token plain\"\u003elayers\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e.\u003c/span\u003e\u003cspan class=\"token plain\"\u003eBatchNormalization\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e(\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e)\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e\u003cspan class=\"token plain\"\u003e\u003c/span\u003e\u003c/div\u003e\u003cdiv class=\"token-line\"\u003e\u003cspan class=\"token plain\"\u003elayers\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e.\u003c/span\u003e\u003cspan class=\"token plain\"\u003eMaxPooling2D\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e(\u003c/span\u003e\u003cspan class=\"token plain\"\u003epool_size\u003c/span\u003e\u003cspan class=\"token operator\"\u003e=\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e(\u003c/span\u003e\u003cspan class=\"token number\"\u003e2\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e\u003cspan class=\"token number\"\u003e2\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e)\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e)\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e\u003cspan class=\"token plain\"\u003e\u003c/span\u003e\u003c/div\u003e\u003cdiv class=\"token-line\"\u003e\u003cspan class=\"token plain\"\u003elayers\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e.\u003c/span\u003e\u003cspan class=\"token plain\"\u003eConv2D\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e(\u003c/span\u003e\u003cspan class=\"token number\"\u003e128\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e\u003cspan class=\"token plain\"\u003e \u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e(\u003c/span\u003e\u003cspan class=\"token number\"\u003e3\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e\u003cspan class=\"token number\"\u003e3\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e)\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e\u003cspan class=\"token plain\"\u003e activation\u003c/span\u003e\u003cspan class=\"token operator\"\u003e=\u003c/span\u003e\u003cspan class=\"token string\"\u003e\u0026quot;relu\u0026quot;\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e)\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e\u003cspan class=\"token plain\"\u003e\u003c/span\u003e\u003c/div\u003e\u003cdiv class=\"token-line\"\u003e\u003cspan class=\"token plain\"\u003elayers\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e.\u003c/span\u003e\u003cspan class=\"token plain\"\u003eBatchNormalization\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e(\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e)\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e\u003cspan class=\"token plain\"\u003e\u003c/span\u003e\u003c/div\u003e\u003cdiv class=\"token-line\"\u003e\u003cspan class=\"token plain\"\u003elayers\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e.\u003c/span\u003e\u003cspan class=\"token plain\"\u003eMaxPooling2D\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e(\u003c/span\u003e\u003cspan class=\"token plain\"\u003epool_size\u003c/span\u003e\u003cspan class=\"token operator\"\u003e=\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e(\u003c/span\u003e\u003cspan class=\"token number\"\u003e1\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e\u003cspan class=\"token number\"\u003e1\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e)\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e)\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e\u003cspan class=\"token plain\"\u003e\u003c/span\u003e\u003c/div\u003e\u003cdiv class=\"token-line\"\u003e\u003cspan class=\"token plain\"\u003elayers\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e.\u003c/span\u003e\u003cspan class=\"token plain\"\u003eConv2D\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e(\u003c/span\u003e\u003cspan class=\"token number\"\u003e256\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e\u003cspan class=\"token plain\"\u003e \u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e(\u003c/span\u003e\u003cspan class=\"token number\"\u003e3\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e\u003cspan class=\"token number\"\u003e3\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e)\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e\u003cspan class=\"token plain\"\u003e activation\u003c/span\u003e\u003cspan class=\"token operator\"\u003e=\u003c/span\u003e\u003cspan class=\"token string\"\u003e\u0026quot;relu\u0026quot;\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e)\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e\u003cspan class=\"token plain\"\u003e\u003c/span\u003e\u003c/div\u003e\u003cdiv class=\"token-line\"\u003e\u003cspan class=\"token plain\"\u003elayers\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e.\u003c/span\u003e\u003cspan class=\"token plain\"\u003eBatchNormalization\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e(\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e)\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e\u003cspan class=\"token plain\"\u003e\u003c/span\u003e\u003c/div\u003e\u003cdiv class=\"token-line\"\u003e\u003cspan class=\"token plain\"\u003elayers\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e.\u003c/span\u003e\u003cspan class=\"token plain\"\u003eMaxPooling2D\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e(\u003c/span\u003e\u003cspan class=\"token plain\"\u003epool_size\u003c/span\u003e\u003cspan class=\"token operator\"\u003e=\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e(\u003c/span\u003e\u003cspan class=\"token number\"\u003e1\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e\u003cspan class=\"token number\"\u003e1\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e)\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e)\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e\u003cspan class=\"token plain\"\u003e\u003c/span\u003e\u003c/div\u003e\u003cdiv class=\"token-line\"\u003e\u003cspan class=\"token plain\"\u003elayers\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e.\u003c/span\u003e\u003cspan class=\"token plain\"\u003eDropout\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e(\u003c/span\u003e\u003cspan class=\"token number\"\u003e0.05\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e)\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e\u003cspan class=\"token plain\"\u003e\u003c/span\u003e\u003c/div\u003e\u003cdiv class=\"token-line\"\u003e\u003cspan class=\"token plain\"\u003elayers\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e.\u003c/span\u003e\u003cspan class=\"token plain\"\u003eFlatten\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e(\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e)\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e\u003cspan class=\"token plain\"\u003e\u003c/span\u003e\u003c/div\u003e\u003cdiv class=\"token-line\"\u003e\u003cspan class=\"token plain\"\u003elayers\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e.\u003c/span\u003e\u003cspan class=\"token plain\"\u003eDense\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e(\u003c/span\u003e\u003cspan class=\"token number\"\u003e512\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e\u003cspan class=\"token plain\"\u003e activation\u003c/span\u003e\u003cspan class=\"token operator\"\u003e=\u003c/span\u003e\u003cspan class=\"token string\"\u003e\u0026quot;relu\u0026quot;\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e)\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e\u003cspan class=\"token plain\"\u003e\u003c/span\u003e\u003c/div\u003e\u003cdiv class=\"token-line\"\u003e\u003cspan class=\"token plain\"\u003elayers\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e.\u003c/span\u003e\u003cspan class=\"token plain\"\u003eDroput\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e(\u003c/span\u003e\u003cspan class=\"token number\"\u003e0.15\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e)\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e\u003cspan class=\"token plain\"\u003e\u003c/span\u003e\u003c/div\u003e\u003cdiv class=\"token-line\"\u003e\u003cspan class=\"token plain\"\u003elayers\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e.\u003c/span\u003e\u003cspan class=\"token plain\"\u003eDense\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e(\u003c/span\u003e\u003cspan class=\"token number\"\u003e256\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e\u003cspan class=\"token plain\"\u003e activation\u003c/span\u003e\u003cspan class=\"token operator\"\u003e=\u003c/span\u003e\u003cspan class=\"token string\"\u003e\u0026quot;relu\u0026quot;\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e)\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e\u003cspan class=\"token plain\"\u003e\u003c/span\u003e\u003c/div\u003e\u003cdiv class=\"token-line\"\u003e\u003cspan class=\"token plain\"\u003elayers\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e.\u003c/span\u003e\u003cspan class=\"token plain\"\u003eDropout\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e(\u003c/span\u003e\u003cspan class=\"token number\"\u003e0.15\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e)\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e\u003cspan class=\"token plain\"\u003e\u003c/span\u003e\u003c/div\u003e\u003cdiv class=\"token-line\"\u003e\u003cspan class=\"token plain\"\u003elayers\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e.\u003c/span\u003e\u003cspan class=\"token plain\"\u003eDense\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e(\u003c/span\u003e\u003cspan class=\"token number\"\u003e37\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e\u003cspan class=\"token plain\"\u003e activation\u003c/span\u003e\u003cspan class=\"token operator\"\u003e=\u003c/span\u003e\u003cspan class=\"token string\"\u003e\u0026quot;sigmoid\u0026quot;\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e)\u003c/span\u003e\u003c/div\u003e\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eThis configuration resulted in \u003cstrong\u003e848,133\u003c/strong\u003e total trainable parameters.\u003c/p\u003e\n\u003ch3\u003e\u003ca href=\"#convolutional-layers\" id=\"convolutional-layers\" class=\"header-link\"\u003eConvolutional Layers\u003c/a\u003e\u003c/h3\u003e\u003cp\u003eConvolutional layers are composed of kernels, or \u003ca href=https://en.wikipedia.org/wiki/Kernel_(image_processing) target=\"_blank\" rel=\"noopener noreferrer\" class=\"link_underline__J1V8J\"\u003eimage filters\u003c/a\u003e. Filters pass over an image in steps, creating a new image where each pixel is the convolution of the filter matrix and a subsection of the image. \u003ca href=https://en.wikipedia.org/wiki/Convolution target=\"_blank\" rel=\"noopener noreferrer\" class=\"link_underline__J1V8J\"\u003eConvolution\u003c/a\u003e is a matrix operation that is in essence a weighted combination of two matrices. The result of this operation is called a \u003cstrong\u003efeature map\u003c/strong\u003e. The feature map is essentially the output of one filter applied to the previous layer.\u003cbr\u003e\u003cimg src=\"/galaxyclass/featuremap.png\" alt=\"featuremap\"\u003e \u003cem\u003eExample of a feature map for recognising handwritten digits\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eA given filter is drawn across the entire previous layer, moved one pixel at a time, with each position resulting in an activation of a neuron and the output being collected in the feature map. These filters allow the network to analyze local relationships between pixels rather than looking at the entire image. One typical filter is a sharpening filter, in 3x3 form here, where \u003cstrong\u003en\u003c/strong\u003e is the level of sharpening to be applied.\u003c/p\u003e\n\u003cp\u003e  0   -1   0\u003c/p\u003e\n\u003cp\u003e  -1   n   -1\u003c/p\u003e\n\u003cp\u003e  0   -1   0\u003c/p\u003e\n\u003cp\u003eAnyone familiar with image processing will recognize this kernel. What makes a CNN powerful is that the convolutional layers are built from neurons themselves which will in effect learn new filters that are uniquely fitted to identifying and understanding features in the images. It will likely learn an edge detection filter and other standard filters, but it will also, as the network depth increases, learn unique filters that pick up subtle details.\u003c/p\u003e\n\u003ch3\u003e\u003ca href=\"#pooling\" id=\"pooling\" class=\"header-link\"\u003ePooling\u003c/a\u003e\u003c/h3\u003e\u003cp\u003eAfter each convolutional layer, we are increasing the width of our network, that is the number of filtered images. As we add on layers, we begin to suffer from the sheer dimensionality of the network, with the number of neurons skyrocketing. This becomes increasingly computationally expensive, and keeping that extra data hurts actually more than it helps. Because our objective with the learned kernels is to recognize patterns, reducing the size of the matrices effectively allows the neural network to zoom out and look at relationships between features recognized at previous convolution steps.\u003cbr\u003eRemember that our ultimate goal is to understand the relationship between the pixels in an image and turn that information into 37 answers to 11 questions. As information flows through the network, higher level features are being extracted and it is at this point that it is important to recognize a wider variety of features without getting bogged down in tiny details. This is where Pooling comes in. Pooling layers decrease the size of our matrices by breaking them down into grids, say 2x2, and reducing them each to a single pixel. There are a few strategies, like max, sum, average, etc, with each having their own strenghts and benefits. I chose to use max pooling, which kind of performs like a sharpening filter, by just taking the maximum value for each patch of the feature map.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://d2l.ai/_images/pooling.svg\" alt=\"asd\"\u003e\u003c/p\u003e\n\u003ch3\u003e\u003ca href=\"#data-augmentation\" id=\"data-augmentation\" class=\"header-link\"\u003eData Augmentation\u003c/a\u003e\u003c/h3\u003e\u003cp\u003eData augmentation is a popular technique to artificially increase the diversity of your training set by applying random (but realistic) transformations, such as image rotation, scaling and flipping. I decided to use the \u003ca href=https://www.tensorflow.org/tutorials/images/data_augmentation target=\"_blank\" rel=\"noopener noreferrer\" class=\"link_underline__J1V8J\"\u003epreprocessing layers\u003c/a\u003e built into TensorFlow to achieve this. Their method doesn\u0026#39;t actually increase the size of the dataset like you would expect, but applies the aforementioned transformations in between training. I chose this method over creating completely new images, as my dataset is already large enough.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"prism-code language-python\"\u003e\u003cdiv class=\"token-line\"\u003e\u003cspan class=\"token plain\"\u003elayers\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e.\u003c/span\u003e\u003cspan class=\"token plain\"\u003epreprocessing\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e.\u003c/span\u003e\u003cspan class=\"token plain\"\u003eRandomFlip\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e(\u003c/span\u003e\u003cspan class=\"token string\"\u003e\u0026quot;horizontal_and_vertical\u0026quot;\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e)\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e\u003cspan class=\"token plain\"\u003e\u003c/span\u003e\u003c/div\u003e\u003cdiv class=\"token-line\"\u003e\u003cspan class=\"token plain\"\u003elayers\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e.\u003c/span\u003e\u003cspan class=\"token plain\"\u003epreprocessing\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e.\u003c/span\u003e\u003cspan class=\"token plain\"\u003eRandomRotation\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e(\u003c/span\u003e\u003cspan class=\"token number\"\u003e0.3\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e)\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e\u003cspan class=\"token plain\"\u003e\u003c/span\u003e\u003c/div\u003e\u003cdiv class=\"token-line\"\u003e\u003cspan class=\"token plain\"\u003elayers\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e.\u003c/span\u003e\u003cspan class=\"token plain\"\u003epreprocessing\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e.\u003c/span\u003e\u003cspan class=\"token plain\"\u003eRandomZoom\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e(\u003c/span\u003e\u003cspan class=\"token plain\"\u003eheight_factor\u003c/span\u003e\u003cspan class=\"token operator\"\u003e=\u003c/span\u003e\u003cspan class=\"token number\"\u003e0.2\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e\u003cspan class=\"token plain\"\u003e width_factor\u003c/span\u003e\u003cspan class=\"token operator\"\u003e=\u003c/span\u003e\u003cspan class=\"token number\"\u003e0.2\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e)\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e\u003cspan class=\"token plain\"\u003e\u003c/span\u003e\u003c/div\u003e\u003cdiv class=\"token-line\"\u003e\u003cspan class=\"token plain\"\u003elayers\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e.\u003c/span\u003e\u003cspan class=\"token plain\"\u003epreprocessing\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e.\u003c/span\u003e\u003cspan class=\"token plain\"\u003eRandomContrast\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e(\u003c/span\u003e\u003cspan class=\"token number\"\u003e0.05\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e)\u003c/span\u003e\u003cspan class=\"token punctuation\"\u003e,\u003c/span\u003e\u003c/div\u003e\u003c/code\u003e\u003c/pre\u003e\u003ch3\u003e\u003ca href=\"#training\" id=\"training\" class=\"header-link\"\u003eTraining\u003c/a\u003e\u003c/h3\u003e\u003cp\u003eAfter some consideration I settled on training my CNN with the Adam optimizer. You could think of Adam as an extension to classical stochastic gradient descent. While traditional SGD maintains a single learning rate for all weight updates and does not adapt during training, Adam and other variants of SGD \u003cstrong\u003ecan\u003c/strong\u003e adjust the learning rate on the fly during training, which lets them achieve much faster convergence speeds than vanilla SGD. I still don\u0026#39;t really know how to optimize their parameters and certainly can\u0026#39;t appreciate how they work on a deeper mathematical level, but that is beyond the scope of this project. All you really have to know is that Adam is a little bit faster than stochastic gradient descent with comparable performance. Training the full model over 50 epochs took about 7 hours on a GPU-accelerated workstation (RTX 3080 Ti)\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/galaxyclass/finaltrain.png\" alt=\"finaltrain\"\u003e\u003c/p\u003e\n\u003cp\u003eAs you can see in both the RMSE over epoch and loss over epoch, the model converges pretty well. Although I initially had trouble with the model overfitting after around 25 epochs. I solved this by increasing the depth of the network from the previous 3 consecutive convolutional layers to 6. I also saw major improvements after applying data augmentation, which helped the model quite a bit to increase its accuracy after epoch 25. Another key improvement came after finding the right combination of kernel and filter sizes. As it turn out CNNs with multiple consecutive convolutional layers learn best with \u003cstrong\u003eincreasing filter\u003c/strong\u003e and \u003cstrong\u003edecreasing kernel sizes\u003c/strong\u003e. This whole process of finding the right combination of hyperparameters, felt a lot like \u003ca href=https://xkcd.com/1838/ target=\"_blank\" rel=\"noopener noreferrer\" class=\"link_underline__J1V8J\"\u003estiring the pile\u003c/a\u003e until it starts looking right. I was initially going to do an empirical approach to optimizing my model, meaning I would write a program that creates a bunch of models with different hyperparametes, trains them for a couple of epochs and then evaluates which combination yielded the best results. But due to time constraints I decided to just go with trial and error. Perhaps this is something I am going to revisit the next time I will build a neural network.\u003c/p\u003e\n\u003ch2\u003e\u003ca href=\"#results\" id=\"results\" class=\"header-link\"\u003eResults\u003c/a\u003e\u003c/h2\u003e\u003cp\u003eMy final result? A respectable score of \u003cstrong\u003e0.0926\u003c/strong\u003e! I would have placed 28th in the original \u003ca href=https://www.kaggle.com/c/galaxy-zoo-the-galaxy-challenge/leaderboard target=\"_blank\" rel=\"noopener noreferrer\" class=\"link_underline__J1V8J\"\u003eGalaxy Zoo competition\u003c/a\u003e. I am more than happy with my final score, as I had limited time to work on this project. Again comparing it to the other methods, the convolutional neural network yielded the lowest deviation. \u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003eMethod\u003c/th\u003e\n\u003cth\u003e\u003c/th\u003e\n\u003cth\u003e\u003c/th\u003e\n\u003cth\u003eRMSE\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\u003ctr\u003e\n\u003ctd\u003eLR (central pixel)\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003ctd\u003e0.1577\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eLR (average pixel)\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003ctd\u003e0.1597\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eCNN\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003ctd\u003e0.0926\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\u003c/table\u003e\n\u003cp\u003eCNNs are once again proving to be a good machine learning method for finding patterns in image data.\u003c/p\u003e\n\u003ch2\u003e\u003ca href=\"#installation\" id=\"installation\" class=\"header-link\"\u003eInstallation\u003c/a\u003e\u003c/h2\u003e\u003cp\u003eDownload the source code\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"prism-code language-bash\"\u003e\u003cdiv class=\"token-line\"\u003e\u003cspan class=\"token plain\"\u003e$ \u003c/span\u003e\u003cspan class=\"token function\"\u003egit\u003c/span\u003e\u003cspan class=\"token plain\"\u003e clone https://github.com/paulphys/galaxyclass\u003c/span\u003e\u003c/div\u003e\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eThe source code already includes instructions for downloading the dataset, but in case you want to get it separately\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"prism-code language-bash\"\u003e\u003cdiv class=\"token-line\"\u003e\u003cspan class=\"token plain\"\u003e$ \u003c/span\u003e\u003cspan class=\"token function\"\u003ecurl\u003c/span\u003e\u003cspan class=\"token plain\"\u003e -LO https://physics.sh/galaxyclass-data.zip\u003c/span\u003e\u003c/div\u003e\u003c/code\u003e\u003c/pre\u003e\u003ch2\u003e\u003ca href=\"#usage\" id=\"usage\" class=\"header-link\"\u003eUsage\u003c/a\u003e\u003c/h2\u003e\u003cp\u003eThis is what the file structure of the provided repository looks like.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/galaxyclass/filetree.png\" alt=\"filetree\"\u003e\u003c/p\u003e\n\u003cp\u003eAll of the code for this project lives in the \u003ccode\u003ecode/galaxyclass.ipynb\u003c/code\u003e Jupyter Notebook, so make sure you have \u003ca href=https://jupyter.org/install target=\"_blank\" rel=\"noopener noreferrer\" class=\"link_underline__J1V8J\"\u003eJupyter\u003c/a\u003e installed before proceeding. Inside of \u003ccode\u003edata/models/\u003c/code\u003e you can find my final trained model and the folder \u003ccode\u003ereport/\u003c/code\u003e contains this document and the HTML version of the notebook.\u003c/p\u003e\n\u003ch3\u003e\u003ca href=\"#jupyter-notebook\" id=\"jupyter-notebook\" class=\"header-link\"\u003eJupyter Notebook\u003c/a\u003e\u003c/h3\u003e\u003cp\u003eStart the notebook\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"prism-code language-bash\"\u003e\u003cdiv class=\"token-line\"\u003e\u003cspan class=\"token plain\"\u003e$ jupyter-notebook galaxyclass.ipynb\u003c/span\u003e\u003c/div\u003e\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eFrom here you just have to execute all cells going from top to bottom, starting with the installation of the dependencies and hardware initialization. If you plan to train the model for yourself, I highly recommend using GPU acceleration, otherwise it is going to take ages to train on the CPU. For that I also included two methods for allocating GPU memory, with the latter restricting access to a predefined amount of memory, in case you want/have to share resources between multiple users or processes.\u003c/p\u003e\n\u003ch2\u003e\u003ca href=\"#conclusion\" id=\"conclusion\" class=\"header-link\"\u003eConclusion\u003c/a\u003e\u003c/h2\u003e\u003cp\u003eI had plenty of fun working on this project. This definitely sparked my interests going into computational physics and I feel very grateful to have had this opportunity. Given more time I would experiment more with both the preprocessing, particularly with the region of interest detection cropping off galaxies at the far edge, as well as exploring other network architectures, like \u003cstrong\u003eResNet\u003c/strong\u003e, which have been shown to outperform traditional CNNs. To further improve this model I could also try to implement feature extraction to train the model only on selected features rather than all pixels at once.\u003cbr\u003eAll in all, I gained some interesting insights into the computational methods involved in physics research and I am looking forward to doing more of it in the future.\u003c/p\u003e\n\u003ch2\u003e\u003ca href=\"#references\" id=\"references\" class=\"header-link\"\u003eReferences\u003c/a\u003e\u003c/h2\u003e\u003cp\u003e[\u003cstrong\u003e1\u003c/strong\u003e] Galaxy Zoo : Morphologies derived from visual inspection of galaxies from the Sloan Digital Sky Survey\u003cbr\u003e\u003ca href=https://arxiv.org/abs/0804.4483 target=\"_blank\" rel=\"noopener noreferrer\" class=\"link_underline__J1V8J\"\u003ehttps://arxiv.org/abs/0804.4483\u003c/a\u003e\u003cbr\u003e[\u003cstrong\u003e2\u003c/strong\u003e] Galaxy Zoo 2: detailed morphological classifications for 304,122 galaxies from the Sloan Digital Sky Survey\u003cbr\u003e \u003ca href=https://arxiv.org/abs/1308.3496 target=\"_blank\" rel=\"noopener noreferrer\" class=\"link_underline__J1V8J\"\u003ehttps://arxiv.org/abs/1308.3496\u003c/a\u003e\u003cbr\u003e[\u003cstrong\u003e3\u003c/strong\u003e] Galaxy Zoo - The Galaxy Challenge\u003cbr\u003e\u003ca href=https://www.kaggle.com/c/galaxy-zoo-the-galaxy-challenge/ target=\"_blank\" rel=\"noopener noreferrer\" class=\"link_underline__J1V8J\"\u003ehttps://www.kaggle.com/c/galaxy-zoo-the-galaxy-challenge/\u003c/a\u003e\u003cbr\u003e[\u003cstrong\u003e4\u003c/strong\u003e] Maximum Pooling and Average Pooling\u003cbr\u003e\u003ca href=https://d2l.ai/chapter_convolutional-neural-networks/pooling.html target=\"_blank\" rel=\"noopener noreferrer\" class=\"link_underline__J1V8J\"\u003ehttps://d2l.ai/chapter_convolutional-neural-networks/pooling.html\u003c/a\u003e\u003cbr\u003e[\u003cstrong\u003e5\u003c/strong\u003e] Visualizing Feature Maps and Filters\u003cbr\u003e\u003ca href=https://medium.com/dataseries/visualizing-the-feature-maps-and-filters-by-convolutional-neural-networks-e1462340518e target=\"_blank\" rel=\"noopener noreferrer\" class=\"link_underline__J1V8J\"\u003ehttps://medium.com/dataseries/visualizing-the-feature-maps-and-filters-by-convolutional-neural-networks-e1462340518e\u003c/a\u003e\u003c/p\u003e\n"},"__N_SSG":true},"page":"/projects/[slug]","query":{"slug":"galaxyclass"},"buildId":"mcDJhOfK6FMbFSF-6hBRB","isFallback":false,"gsp":true,"scriptLoader":[]}</script><next-route-announcer><p aria-live="assertive" id="__next-route-announcer__" role="alert" style="border: 0px; clip: rect(0px, 0px, 0px, 0px); height: 1px; margin: -1px; overflow: hidden; padding: 0px; position: absolute; width: 1px; white-space: nowrap; overflow-wrap: normal;"></p></next-route-announcer></body></html>